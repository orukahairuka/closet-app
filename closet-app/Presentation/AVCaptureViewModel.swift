//
//  AVCaptureViewModel.swift
//  closet-app
//
//  Created by æ«»äº•çµµç†é¦™ on 2025/06/20.
//

import AVFoundation
import Combine
import Foundation
import UIKit
import Vision

extension AVCaptureViewModel {

    func captureStillImage(completion: @escaping (UIImage?) -> Void) {
        print("ğŸ“¸ captureStillImage called")
        let settings = AVCapturePhotoSettings(format: [AVVideoCodecKey: AVVideoCodecType.jpeg])
        settings.isHighResolutionPhotoEnabled = true

        // delegate ã‚’ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã«ä¿æŒ
        let delegate = PhotoCaptureDelegate { [weak self] image in
            completion(image)
            self?.photoCaptureDelegate = nil // è§£æ”¾
        }

        self.photoCaptureDelegate = delegate
        photoOutput.capturePhoto(with: settings, delegate: delegate)
    }
}

private class PhotoCaptureDelegate: NSObject, AVCapturePhotoCaptureDelegate {
    let completion: (UIImage?) -> Void

    init(completion: @escaping (UIImage?) -> Void) {
        print("ğŸ›  PhotoCaptureDelegate initialized")
        self.completion = completion
    }

    func photoOutput(_ output: AVCapturePhotoOutput,
                     didFinishProcessingPhoto photo: AVCapturePhoto,
                     error: Error?) {
        print("ğŸ“¥ didFinishProcessingPhoto called")

        if let error = error {
            print("âŒ å†™çœŸå–å¾—ã‚¨ãƒ©ãƒ¼: \(error.localizedDescription)")
        }

        if let data = photo.fileDataRepresentation() {
            print("ğŸ“¦ ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: \(data.count) bytes")
            if let image = UIImage(data: data) {
                print("âœ… ç”»åƒå–å¾—æˆåŠŸ")
                completion(image)
            } else {
                print("âŒ UIImageå¤‰æ›å¤±æ•—")
                completion(nil)
            }
        } else {
            print("âŒ dataãŒnil")
            completion(nil)
        }

    }
}

public class AVCaptureViewModel: NSObject, ObservableObject, AVCaptureVideoDataOutputSampleBufferDelegate {

    public var captureSession = AVCaptureSession()
    private var mlRequest = [VNRequest]()

    private let photoOutput = AVCapturePhotoOutput() // â† ã“ã“ã§æŒãŸã›ã‚‹

    @Published var identifier: String?
    @Published var confidence: Float?
    @Published var resultsText: String?
    @Published var resultsImage: String? = "sonota"
    @Published var image: UIImage?
    private var photoCaptureDelegate: PhotoCaptureDelegate? // âœ… ã“ã“ã«ç§»å‹•ï¼


    public override init() {
        super.init()
        setupSession()
        _ = setupVision()
    }

    func setupSession() {
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let self else { return }
            captureSession.beginConfiguration()

            // å…¥åŠ›ï¼ˆã‚«ãƒ¡ãƒ©ï¼‰
            guard let device = AVCaptureDevice.default(for: .video),
                  let input = try? AVCaptureDeviceInput(device: device),
                  captureSession.canAddInput(input) else {
                captureSession.commitConfiguration()
                return
            }
            captureSession.addInput(input)

            // å‡ºåŠ›ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ˜ åƒï¼‰
            let videoOutput = AVCaptureVideoDataOutput()
            videoOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "camera.queue"))
            guard captureSession.canAddOutput(videoOutput) else {
                captureSession.commitConfiguration()
                return
            }
            captureSession.addOutput(videoOutput)

            // âœ… å†™çœŸå‡ºåŠ›ï¼ˆã“ã“é‡è¦ï¼‰
            if captureSession.canAddOutput(photoOutput) {
                captureSession.addOutput(photoOutput)
                photoOutput.isHighResolutionCaptureEnabled = true // â† è¿½åŠ ï¼
            }

            captureSession.commitConfiguration()
            captureSession.startRunning()
        }
    }




    func setupVision() -> NSError? {
        guard let modelURL = Bundle.main.url(forResource: "TestModel", withExtension: "mlmodelc") else {
            return NSError(domain: "Vision", code: -1, userInfo: [NSLocalizedDescriptionKey: "ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"])
        }

        do {
            let model = try VNCoreMLModel(for: MLModel(contentsOf: modelURL))
            let request = VNCoreMLRequest(model: model) { request, _ in
                DispatchQueue.main.async {
                    if let results = request.results as? [VNClassificationObservation] {
                        print("ğŸ” Visionçµæœ: \(results.map { "\($0.identifier)(\($0.confidence))" })")
                        self.handleResults(results)
                    } else {
                        print("âŒ Visionçµæœå¤‰æ›å¤±æ•—")
                    }
                }
            }

            self.mlRequest = [request]
        } catch {
            print("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼:", error)
        }
        return nil
    }

    private var isProcessing = false

    private var lastVisionRun = Date(timeIntervalSince1970: 0)

    public func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        let now = Date()
        guard now.timeIntervalSince(lastVisionRun) > 0.5 else {
            return // å‰å›ã‹ã‚‰0.5ç§’æœªæº€ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
        }
        lastVisionRun = now

        guard let buffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {
            return
        }

        let ciImage = CIImage(cvImageBuffer: buffer)
        let orientation = CGImagePropertyOrientation.up
        let handler = VNImageRequestHandler(ciImage: ciImage, orientation: orientation, options: [:])

        DispatchQueue.global(qos: .userInitiated).async {
            do {
                print("ğŸ§  Visionãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
                try handler.perform(self.mlRequest)
            } catch {
                print("âŒ Visionã‚¨ãƒ©ãƒ¼:", error)
            }
        }
    }




    private func handleResults(_ results: [Any]) {
        guard let observation = results.first as? VNClassificationObservation else {
            resultsText = "åˆ†é¡ã§ãã¾ã›ã‚“ã§ã—ãŸ"
            return
        }

        identifier = observation.identifier
        confidence = floor(observation.confidence * 100)

        // ä¿¡é ¼åº¦ãŒä½ã„å ´åˆã¯ accessory æ‰±ã„ã«ã™ã‚‹
        if observation.confidence < 0.7 {
            resultsText = "ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“"
            resultsImage = "accessory"
            return
        }

        switch observation.identifier {
        case "tops":      resultsText = "ãƒˆãƒƒãƒ—ã‚¹ã§ã™ï¼"
        case "bottoms":   resultsText = "ãƒœãƒˆãƒ ã‚¹ã§ã™ï¼"
        case "outer":     resultsText = "ã‚¢ã‚¦ã‚¿ãƒ¼ã§ã™ï¼"
        case "dress":     resultsText = "ãƒ¯ãƒ³ãƒ”ãƒ¼ã‚¹ã§ã™ï¼"
        case "shoes":     resultsText = "é´ã§ã™ï¼"
        case "bag":       resultsText = "ãƒãƒƒã‚°ã§ã™ï¼"
        default:
            resultsText = "åˆ†é¡ã§ãã¾ã›ã‚“ã§ã—ãŸ"
        }
    }


}
